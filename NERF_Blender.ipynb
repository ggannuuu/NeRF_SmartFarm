{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2VVI9a1NR63Y"
      },
      "outputs": [],
      "source": [
        "# Todo\n",
        "# 데이터로더에서 하나의 어레이로 합치고, batch 사이즈로 다시 맞추는 코드 만들기\n",
        "# 레이 프로세싱할때 ray_o, ray_d 다시 고치기\n",
        "# 데이터 가져올때 np.load로 가져오고 plt으로 나타내기\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yi4Kks0cApKI"
      },
      "source": [
        "# Code Initiation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4mSVwpo8-_iS",
        "outputId": "bd3468d0-8027-4d49-b774-035beffb9c63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting kornia\n",
            "  Downloading kornia-0.7.2-py2.py3-none-any.whl (825 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m825.4/825.4 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting kornia-rs>=0.1.0 (from kornia)\n",
            "  Downloading kornia_rs-0.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from kornia) (24.0)\n",
            "Requirement already satisfied: torch>=1.9.1 in /usr/local/lib/python3.10/dist-packages (from kornia) (2.3.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->kornia) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->kornia) (4.12.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->kornia) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->kornia) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->kornia) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->kornia) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.9.1->kornia)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.9.1->kornia)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.9.1->kornia)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.9.1->kornia)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.9.1->kornia)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.9.1->kornia)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.9.1->kornia)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.9.1->kornia)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.9.1->kornia)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.9.1->kornia)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.9.1->kornia)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->kornia) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.9.1->kornia)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m56.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.9.1->kornia) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.9.1->kornia) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, kornia-rs, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, kornia\n",
            "Successfully installed kornia-0.7.2 kornia-rs-0.1.3 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105\n"
          ]
        }
      ],
      "source": [
        "!pip install kornia\n",
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "from skimage import io, transform\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils\n",
        "from torchvision import transforms as T\n",
        "from google.colab import drive\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "import json\n",
        "import imageio\n",
        "from tqdm import tqdm\n",
        "import torch.nn as nn\n",
        "from PIL import Image\n",
        "# Ignore warnings\n",
        "import warnings\n",
        "from kornia import create_meshgrid\n",
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Helper Functions"
      ],
      "metadata": {
        "id": "lZBgY0DQftHd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_folder(folder_path):\n",
        "    if not os.path.exists(folder_path):\n",
        "        os.makedirs(folder_path)\n",
        "        print(f\"Folder created: {folder_path}\")\n",
        "    else:\n",
        "        print(f\"Folder already exists: {folder_path}\")\n",
        "\n",
        "def save_model(model, optimizer, path, name, lr, nb_epochs):\n",
        "  path = path + \"models/\"\n",
        "  create_folder(path)\n",
        "  #current_time = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "  model_save_path = f'{path}nerf_model_{name}_lr{lr}_epochs{nb_epochs}.pth'\n",
        "  optimizer_save_path = f'{path}nerf_optimizer_{name}_lr{lr}_epochs{nb_epochs}.pth'\n",
        "  torch.save(model.state_dict(), model_save_path)\n",
        "  torch.save(model_optimizer.state_dict(), optimizer_save_path)\n",
        "  print(\"NeRF Model Saved Successfully\")\n",
        "\n",
        "def load_model(model_path, opt_path):\n",
        "  model.load_state_dict(torch.load(model_path))\n",
        "  model_optimizer.load_state_dict(torch.load(opt_path))\n",
        "  return\n",
        "\n"
      ],
      "metadata": {
        "id": "uytD7-DYfv6s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VsfW-5BuAyKX"
      },
      "source": [
        "# Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "poKzLRVtA0ou"
      },
      "outputs": [],
      "source": [
        "class ChairDataset(Dataset):\n",
        "\n",
        "    #init function\n",
        "    def __init__(self, datadir, json_dir = 'transforms_train.json', img_dir = 'train/', batch_size=256, H=400, W=400):\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "            datadir (string): Directory to all of the training and testing data\n",
        "            json_dir (string): Path to the json file with annotations.\n",
        "            img_dir (string): Directory with all the images.\n",
        "            transform (callable, optional): Optional transform to be applied\n",
        "                on a sample.\n",
        "            file_path (string): Path to each of the image\n",
        "            rotation (f): Rotation value of the each of the image\n",
        "            transform_matrix (list): 4d array\n",
        "        \"\"\"\n",
        "\n",
        "        #processing the data\n",
        "        f = open(datadir + json_dir, \"r\")\n",
        "        data_json = json.loads(f.read())\n",
        "        data_json_dict = data_json['frames']\n",
        "        N = len(data_json_dict)\n",
        "        file_path = []\n",
        "        rotation = []\n",
        "        transform_matrix = []\n",
        "        camera_angle_x = data_json['camera_angle_x']\n",
        "\n",
        "        data_collection = [file_path, transform_matrix]\n",
        "\n",
        "        for i in range(N):\n",
        "          cnt = 0\n",
        "          for key in data_json_dict[i]:\n",
        "            data_collection[cnt].append(data_json_dict[i][key])\n",
        "            cnt += 1\n",
        "\n",
        "        rays_origin = []\n",
        "        rays_direction = []\n",
        "        pixel_value = []\n",
        "\n",
        "\n",
        "        for idx in range(N):\n",
        "\n",
        "          img_name = datadir + file_path[idx].replace('\\\\', '/')\n",
        "\n",
        "          trans_mat = transform_matrix[idx]\n",
        "\n",
        "          rays_o_, rays_d_, target_px_values_ = read_data(img_name, trans_mat, camera_angle_x, H, W)\n",
        "\n",
        "          rays_origin += [rays_o_]\n",
        "          rays_direction += [rays_d_]\n",
        "          pixel_value += [target_px_values_]\n",
        "\n",
        "\n",
        "\n",
        "        rays_o = torch.cat(rays_origin)\n",
        "        rays_d = torch.cat(rays_direction)\n",
        "        target_px_values = torch.cat(pixel_value)\n",
        "        print(len(rays_o))\n",
        "        print(len(target_px_values))\n",
        "\n",
        "\n",
        "\n",
        "        #Chair Dataset Variables\n",
        "        self.size = rays_o.shape[0]\n",
        "        self.H = H\n",
        "        self.W = W\n",
        "        self.batch_size = batch_size\n",
        "        self.rays_o = rays_o\n",
        "        self.rays_d = rays_d\n",
        "        self.target_px_values = target_px_values\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return (self.size + self.batch_size - 1) // self.batch_size\n",
        "\n",
        "    def __H__(self):\n",
        "        return self.H\n",
        "    def __W__(self):\n",
        "        return self.W\n",
        "\n",
        "    # Get Item: returns a sample which contains image (file path to each image), rotation (int), and transform matrix ((4, 4) ndarray)\n",
        "    def __getitem__(self, idx):\n",
        "        start = idx * self.batch_size\n",
        "        end = min(start + self.batch_size, self.size)\n",
        "\n",
        "        batch_rays_o = self.rays_o[start:end]\n",
        "        batch_rays_d = self.rays_d[start:end]\n",
        "        batch_px = self.target_px_values[start:end]\n",
        "\n",
        "        # if (np.any(batch_px > 1)):\n",
        "        #   print(\"Not Normalized\")\n",
        "\n",
        "        sample = {'rays_o': batch_rays_o,\n",
        "                  'rays_d': batch_rays_d,\n",
        "                  'target_px_values': batch_px}\n",
        "\n",
        "        return sample\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SXTR0NCgBy5m"
      },
      "outputs": [],
      "source": [
        "def read_data(image_dir, pose, camera_angle_x, H, W):\n",
        "    to_tensor = T.ToTensor()\n",
        "\n",
        "    focal = 0.5*800/np.tan(0.5*camera_angle_x) # original focal length\n",
        "                                                                  # when W=800\n",
        "\n",
        "    focal *= W/800 # modify focal length to match size self.img_wh\n",
        "\n",
        "    # bounds, common for all scenes\n",
        "    near = 2.0\n",
        "    far = 6.0\n",
        "    bounds = np.array([near, far])\n",
        "\n",
        "    # ray directions for all pixels, same for all images (same H, W, focal)\n",
        "    directions = \\\n",
        "        get_ray_directions(H, W, focal) # (h, w, 3)\n",
        "\n",
        "\n",
        "    pose = np.array(pose)[:3, :4]\n",
        "\n",
        "    c2w = torch.FloatTensor(pose)\n",
        "\n",
        "\n",
        "    img = Image.open(image_dir)\n",
        "    img = img.resize((W, H), Image.LANCZOS)\n",
        "    if (np.shape(img)[0]==4):\n",
        "      img = to_tensor(img) # (4, h, w)\n",
        "      img = img.view(4, -1).permute(1, 0) # (h*w, 4) RGBA\n",
        "      img = img[:, :3]*img[:, -1:] + (1-img[:, -1:]) # blend A to RGB\n",
        "    else :\n",
        "      img = to_tensor(img) # (3, h, w)\n",
        "      img = img.view(3, -1).permute(1, 0) # (h*w, 3) RGB\n",
        "\n",
        "\n",
        "    rays_o, rays_d = get_rays(directions, c2w)\n",
        "\n",
        "\n",
        "    return rays_o, rays_d, img\n",
        "\n",
        "\n",
        "\n",
        "def get_ray_directions(H, W, focal):\n",
        "\n",
        "    grid = create_meshgrid(H, W, normalized_coordinates=False)[0]\n",
        "    i, j = grid.unbind(-1)\n",
        "    directions = \\\n",
        "        torch.stack([(i-W/2)/focal, -(j-H/2)/focal, -torch.ones_like(i)], -1) # (H, W, 3)\n",
        "\n",
        "    return directions\n",
        "\n",
        "\n",
        "def get_rays(directions, c2w):\n",
        "\n",
        "    # Rotate ray directions from camera coordinate to the world coordinate\n",
        "    rays_d = directions @ c2w[:, :3].T # (H, W, 3)\n",
        "    rays_d = rays_d / torch.norm(rays_d, dim=-1, keepdim=True)\n",
        "    # The origin of all rays is the camera origin in world coordinate\n",
        "    rays_o = c2w[:, 3].expand(rays_d.shape) # (H, W, 3)\n",
        "\n",
        "    rays_d = rays_d.view(-1, 3)\n",
        "    rays_o = rays_o.view(-1, 3)\n",
        "\n",
        "    return rays_o, rays_d"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwPl-4iNOrk5"
      },
      "source": [
        "# Ray Process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FWTYeEMA-jhl"
      },
      "outputs": [],
      "source": [
        "def compute_accumulated_transmittance(alphas):\n",
        "    accumulated_transmittance = torch.cumprod(alphas, 1)\n",
        "    return torch.cat((torch.ones((accumulated_transmittance.shape[0], 1), device=alphas.device),\n",
        "                      accumulated_transmittance[:, :-1]), dim=-1)\n",
        "\n",
        "\n",
        "def render_rays(nerf_model, ray_origins, ray_directions, hn=0, hf=0.5, nb_bins=192):\n",
        "    device = ray_origins.device\n",
        "\n",
        "    t = torch.linspace(hn, hf, nb_bins, device=device).expand(ray_origins.shape[0], nb_bins)\n",
        "    # Perturb sampling along each ray.\n",
        "    mid = (t[:, :-1] + t[:, 1:]) / 2.\n",
        "    lower = torch.cat((t[:, :1], mid), -1)\n",
        "    upper = torch.cat((mid, t[:, -1:]), -1)\n",
        "    u = torch.rand(t.shape, device=device)\n",
        "    t = lower + (upper - lower) * u  # [batch_size, nb_bins]\n",
        "    delta = torch.cat((t[:, 1:] - t[:, :-1], torch.tensor([1e10], device=device).expand(ray_origins.shape[0], 1)), -1)\n",
        "\n",
        "    # Compute the 3D points along each ray\n",
        "    x = ray_origins.unsqueeze(1) + t.unsqueeze(2) * ray_directions.unsqueeze(1)   # [batch_size, nb_bins, 3]\n",
        "    # Expand the ray_directions tensor to match the shape of x\n",
        "    ray_directions = ray_directions.expand(nb_bins, ray_directions.shape[0], 3).transpose(0, 1)\n",
        "\n",
        "    colors, sigma = nerf_model(x.reshape(-1, 3), ray_directions.reshape(-1, 3))\n",
        "    colors = colors.reshape(x.shape)\n",
        "    sigma = sigma.reshape(x.shape[:-1])\n",
        "\n",
        "    alpha = 1 - torch.exp(-sigma * delta)  # [batch_size, nb_bins]\n",
        "    weights = compute_accumulated_transmittance(1 - alpha).unsqueeze(2) * alpha.unsqueeze(2)\n",
        "    # Compute the pixel values as a weighted sum of colors along each ray\n",
        "    c = (weights * colors).sum(dim=1)\n",
        "    weight_sum = weights.sum(-1).sum(-1)  # Regularization for white background\n",
        "    return c + 1 - weight_sum.unsqueeze(-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qk9spqCeOw10"
      },
      "source": [
        "# NeRF Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nzgCr0EuRXF0"
      },
      "outputs": [],
      "source": [
        "class NerfModel(nn.Module):\n",
        "    def __init__(self, embedding_dim_pos=10, embedding_dim_direction=4, hidden_dim=128):\n",
        "        super(NerfModel, self).__init__()\n",
        "\n",
        "        self.block1 = nn.Sequential(nn.Linear(embedding_dim_pos * 6 + 3, hidden_dim), nn.ReLU(),\n",
        "                                    nn.Linear(hidden_dim, hidden_dim), nn.ReLU(),\n",
        "                                    nn.Linear(hidden_dim, hidden_dim), nn.ReLU(),\n",
        "                                    nn.Linear(hidden_dim, hidden_dim), nn.ReLU(), )\n",
        "        # density estimation\n",
        "        self.block2 = nn.Sequential(nn.Linear(embedding_dim_pos * 6 + hidden_dim + 3, hidden_dim), nn.ReLU(),\n",
        "                                    nn.Linear(hidden_dim, hidden_dim), nn.ReLU(),\n",
        "                                    nn.Linear(hidden_dim, hidden_dim), nn.ReLU(),\n",
        "                                    nn.Linear(hidden_dim, hidden_dim + 1), )\n",
        "        # color estimation\n",
        "        self.block3 = nn.Sequential(nn.Linear(embedding_dim_direction * 6 + hidden_dim + 3, hidden_dim // 2), nn.ReLU(), )\n",
        "        self.block4 = nn.Sequential(nn.Linear(hidden_dim // 2, 3), nn.Sigmoid(), )\n",
        "\n",
        "        self.embedding_dim_pos = embedding_dim_pos\n",
        "        self.embedding_dim_direction = embedding_dim_direction\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    @staticmethod\n",
        "    def positional_encoding(x, L):\n",
        "        out = [x]\n",
        "        for j in range(L):\n",
        "            out.append(torch.sin(2 ** j * x))\n",
        "            out.append(torch.cos(2 ** j * x))\n",
        "        return torch.cat(out, dim=1)\n",
        "\n",
        "    def forward(self, o, d):\n",
        "        emb_x = self.positional_encoding(o, self.embedding_dim_pos) # emb_x: [batch_size, embedding_dim_pos * 6]\n",
        "        emb_d = self.positional_encoding(d, self.embedding_dim_direction) # emb_d: [batch_size, embedding_dim_direction * 6]\n",
        "        h = self.block1(emb_x) # h: [batch_size, hidden_dim]\n",
        "        tmp = self.block2(torch.cat((h, emb_x), dim=1)) # tmp: [batch_size, hidden_dim + 1]\n",
        "        h, sigma = tmp[:, :-1], self.relu(tmp[:, -1]) # h: [batch_size, hidden_dim], sigma: [batch_size]\n",
        "        h = self.block3(torch.cat((h, emb_d), dim=1)) # h: [batch_size, hidden_dim // 2]\n",
        "        c = self.block4(h) # c: [batch_size, 3]\n",
        "        return c, sigma\n",
        "\n",
        "def init_weights(m):\n",
        "    if isinstance(m, nn.Linear):\n",
        "        torch.nn.init.xavier_uniform_(m.weight)\n",
        "        if m.bias is not None:\n",
        "            torch.nn.init.zeros_(m.bias)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fhGormneOz2x"
      },
      "source": [
        "# NeRF Test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xfJdG9jI-RIk"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def test(hn, hf, dataset, chunk_size=10, nb_bins=192, H=400, W=400, epoch_idx = 0,\n",
        "         output=\"/content/drive/MyDrive/NeRF_Data_Repository/output\", lr=5e-4):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        hn: near plane distance\n",
        "        hf: far plane distance\n",
        "        dataset: dataset to render\n",
        "        chunk_size (int, optional): chunk size for memory efficiency. Defaults to 10.\n",
        "        img_index (int, optional): image index to render. Defaults to 0.\n",
        "        nb_bins (int, optional): number of bins for density estimation. Defaults to 192.\n",
        "        H (int, optional): image height. Defaults to 400.\n",
        "        W (int, optional): image width. Defaults to 400.\n",
        "\n",
        "    Returns:\n",
        "        None: None\n",
        "    \"\"\"\n",
        "\n",
        "    idx = 0\n",
        "    for batch in dataset:\n",
        "      if idx == len(dataset):\n",
        "        break\n",
        "\n",
        "      ray_origins = batch['rays_o']\n",
        "      ray_directions = batch['rays_d']\n",
        "\n",
        "      data = []   # list of regenerated pixel values\n",
        "      for i in range(int(np.ceil(H / chunk_size))):   # iterate over chunks\n",
        "          # Get chunk of rays\n",
        "          ray_origins_ = ray_origins[i * W * chunk_size: (i + 1) * W * chunk_size].to(device)\n",
        "          ray_directions_ = ray_directions[i * W * chunk_size: (i + 1) * W * chunk_size].to(device)\n",
        "          regenerated_px_values = render_rays(model, ray_origins_, ray_directions_, hn=hn, hf=hf, nb_bins=nb_bins)\n",
        "          if (torch.any(regenerated_px_values > 1)):\n",
        "            print(\"Test Not Normalized\")\n",
        "          data.append(regenerated_px_values)\n",
        "      img = torch.cat(data).data.cpu().numpy().reshape(H, W, 3)\n",
        "\n",
        "      if np.any(img > 1):\n",
        "        print(\"error\")\n",
        "\n",
        "      if (idx % 25 == 0):\n",
        "        plt.figure()\n",
        "        plt.title(\"Test\")\n",
        "        plt.imshow(img)\n",
        "        file_name = f'image_{idx}_epoch_{epoch_idx}_lr_{lr}.png'\n",
        "        save_path = f'{output}/{file_name}'\n",
        "        plt.savefig(save_path, bbox_inches='tight')\n",
        "        plt.close()\n",
        "      idx += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pa8dn8bXO4N6"
      },
      "source": [
        "# NeRF Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GD88SI6m-Uy9"
      },
      "outputs": [],
      "source": [
        "def train(nerf_model, optimizer, scheduler, data_loader, test_loader, device='cuda', hn=0, hf=1, nb_epochs=int(1e5),\n",
        "          nb_bins=192, H=400, W=400, directory=\"/content/drive/MyDrive/NeRF_Data_Repository/\", lr=5e-4):\n",
        "    training_loss = []\n",
        "    epoch_idx = 0\n",
        "    output_dir = directory + \"output\"\n",
        "    create_folder(output_dir)\n",
        "    for _ in tqdm(range(nb_epochs)):\n",
        "\n",
        "        epoch_loss = []\n",
        "        i = 0\n",
        "        data = []\n",
        "        img_idx = 0\n",
        "        for batch in data_loader:\n",
        "            ray_origins = batch['rays_o'].to(device)\n",
        "            ray_directions = batch['rays_d'].to(device)\n",
        "            ground_truth_px_values = batch['target_px_values'].to(device)\n",
        "            if (torch.any(ground_truth_px_values > 1)):\n",
        "              print(\"Input Not Normalized\")\n",
        "            regenerated_px_values = render_rays(nerf_model, ray_origins, ray_directions, hn=hn, hf=hf, nb_bins=nb_bins)\n",
        "            data.append(regenerated_px_values)\n",
        "\n",
        "            loss = ((ground_truth_px_values - regenerated_px_values) ** 2).sum()\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss.append(loss)\n",
        "            if (len(data_loader) == i):\n",
        "              break\n",
        "            i += 1\n",
        "\n",
        "        scheduler.step()\n",
        "        training_loss.append(loss)\n",
        "\n",
        "\n",
        "        test(hn, hf, test_loader, nb_bins=nb_bins, H=H, W=W, epoch_idx = epoch_idx, output=output_dir, lr = lr)\n",
        "        epoch_idx += 1\n",
        "    return training_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LA8-3zalPIrX"
      },
      "source": [
        "# Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B2Ci5tqmmdhK"
      },
      "outputs": [],
      "source": [
        "# batch_size, nb_bins: Increase for precision, but takes longer\n",
        "\n",
        "\n",
        "height = 200\n",
        "width = 200\n",
        "batch_size = 200\n",
        "hidden_dim = 256\n",
        "nb_bins = 200\n",
        "nb_epochs = 20\n",
        "learning_rate = 3e-5\n",
        "near_plane = 20 - 16\n",
        "far_plane = 20 + 16\n",
        "folder_name = \"tomato\"\n",
        "#whiteflower\n",
        "#tomato_tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y6Aa_u5sK57t",
        "outputId": "0d2d41b9-b029-47ea-847d-36bb6dbb0303"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive')\n",
        "data_dir = \"/content/drive/MyDrive/NeRF_Data_Repository/\" + folder_name + \"/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CGjUiKIGK8Zu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4373e9b5-7c33-48d1-b487-f4142030289c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4000000\n",
            "4000000\n",
            "4000000\n",
            "4000000\n"
          ]
        }
      ],
      "source": [
        "train_dir = folder_name + \"_dataset_train/\"\n",
        "test_dir = folder_name + \"_dataset_test/\"\n",
        "train_loader = ChairDataset(datadir = data_dir + train_dir, batch_size = batch_size, img_dir = \"train/\", H = height, W = width)\n",
        "test_loader = ChairDataset(datadir = data_dir + test_dir, batch_size = height*width, img_dir = \"train/\", H = height, W = width)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGcHloXgPMh9"
      },
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G_2gKu20K4p6",
        "outputId": "1d23ed0d-d741-4cd1-9a9c-9379dcf76afe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Folder already exists: /content/drive/MyDrive/NeRF_Data_Repository/tomato/output\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20/20 [2:00:20<00:00, 361.05s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[tensor(0., device='cuda:0', grad_fn=<SumBackward0>), tensor(0., device='cuda:0', grad_fn=<SumBackward0>), tensor(0., device='cuda:0', grad_fn=<SumBackward0>), tensor(0., device='cuda:0', grad_fn=<SumBackward0>), tensor(0., device='cuda:0', grad_fn=<SumBackward0>), tensor(0., device='cuda:0', grad_fn=<SumBackward0>), tensor(0., device='cuda:0', grad_fn=<SumBackward0>), tensor(0., device='cuda:0', grad_fn=<SumBackward0>), tensor(0., device='cuda:0', grad_fn=<SumBackward0>), tensor(0., device='cuda:0', grad_fn=<SumBackward0>), tensor(0., device='cuda:0', grad_fn=<SumBackward0>), tensor(0., device='cuda:0', grad_fn=<SumBackward0>), tensor(0., device='cuda:0', grad_fn=<SumBackward0>), tensor(0., device='cuda:0', grad_fn=<SumBackward0>), tensor(0., device='cuda:0', grad_fn=<SumBackward0>), tensor(0., device='cuda:0', grad_fn=<SumBackward0>), tensor(0., device='cuda:0', grad_fn=<SumBackward0>), tensor(0., device='cuda:0', grad_fn=<SumBackward0>), tensor(0., device='cuda:0', grad_fn=<SumBackward0>), tensor(0., device='cuda:0', grad_fn=<SumBackward0>)]\n"
          ]
        }
      ],
      "source": [
        "device = 'cuda'\n",
        "\n",
        "model = NerfModel(hidden_dim=hidden_dim).to(device)\n",
        "model.apply(init_weights)\n",
        "model_optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "scheduler = torch.optim.lr_scheduler.MultiStepLR(model_optimizer, milestones=[2, 4, 8], gamma=0.5)\n",
        "losses = train(model, model_optimizer, scheduler, train_loader, test_loader, nb_epochs=nb_epochs, device=device, hn= near_plane, hf= far_plane, nb_bins=nb_bins, H=height,W=width, directory = data_dir, lr=learning_rate)\n",
        "print(losses)\n",
        "#plt.plot(np.arange(nb_epochs), losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1_RS0mCeoSnU"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBTYf5h3fz8d"
      },
      "source": [
        "# Trash"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RdfHQLlfFDJS"
      },
      "outputs": [],
      "source": [
        "# def read_data(image_dir, pose, camera_angle_x, H, W):\n",
        "#     # with open(os.path.join(self.root_dir,\n",
        "#     #                         f\"transforms_{self.split}.json\"), 'r') as f:\n",
        "#     #     self.meta = json.load(f)\n",
        "\n",
        "\n",
        "#     focal = 0.5*800/np.tan(0.5*camera_angle_x) # original focal length\n",
        "#                                                                   # when W=800\n",
        "\n",
        "#     focal *= W/800 # modify focal length to match size self.img_wh\n",
        "\n",
        "#     # bounds, common for all scenes\n",
        "#     near = 2.0\n",
        "#     far = 6.0\n",
        "#     bounds = np.array([near, far])\n",
        "\n",
        "#     # ray directions for all pixels, same for all images (same H, W, focal)\n",
        "#     directions = \\\n",
        "#         get_ray_directions(H, W, focal) # (h, w, 3)\n",
        "\n",
        "#     pose = np.array(frame['transform_matrix'])[:3, :4]\n",
        "#     poses += [pose]\n",
        "#     c2w = torch.FloatTensor(pose)\n",
        "\n",
        "\n",
        "#     img = Image.open(image_dir)\n",
        "#     img = img.resize((W, H), Image.LANCZOS)\n",
        "#     img = transform(img) # (4, h, w)\n",
        "#     img = img.view(4, -1).permute(1, 0) # (h*w, 4) RGBA\n",
        "#     img = img[:, :3]*img[:, -1:] + (1-img[:, -1:]) # blend A to RGB\n",
        "#     all_rgbs += [img]\n",
        "\n",
        "#     rays_o, rays_d = get_rays(directions, c2w)\n",
        "\n",
        "\n",
        "#     if split == 'train': # create buffer of all rays and rgb data\n",
        "#         image_paths = []\n",
        "#         poses = []\n",
        "#         all_rays = []\n",
        "#         all_rgbs = []\n",
        "#         for frame in meta['frames']:\n",
        "#             pose = np.array(frame['transform_matrix'])[:3, :4]\n",
        "#             poses += [pose]\n",
        "#             c2w = torch.FloatTensor(pose)\n",
        "\n",
        "#             image_path = os.path.join(root_dir, f\"{frame['file_path']}.png\")\n",
        "#             image_paths += [image_path]\n",
        "#             img = Image.open(image_path)\n",
        "#             img = img.resize(img_wh, Image.LANCZOS)\n",
        "#             img = transform(img) # (4, h, w)\n",
        "#             img = img.view(4, -1).permute(1, 0) # (h*w, 4) RGBA\n",
        "#             img = img[:, :3]*img[:, -1:] + (1-img[:, -1:]) # blend A to RGB\n",
        "#             all_rgbs += [img]\n",
        "\n",
        "#             rays_o, rays_d = get_rays(directions, c2w) # both (h*w, 3)\n",
        "\n",
        "#             all_rays += [torch.cat([rays_o, rays_d,\n",
        "#                                           near*torch.ones_like(rays_o[:, :1]),\n",
        "#                                           far*torch.ones_like(rays_o[:, :1])],\n",
        "#                                           1)] # (h*w, 8)\n",
        "\n",
        "#         all_rays = torch.cat(all_rays, 0) # (len(self.meta['frames])*h*w, 3)\n",
        "#         all_rgbs = torch.cat(all_rgbs, 0) # (len(self.meta['frames])*h*w, 3)\n",
        "\n",
        "\n",
        "# def get_ray_directions(H, W, focal):\n",
        "#     \"\"\"\n",
        "#     Get ray directions for all pixels in camera coordinate.\n",
        "#     Reference: https://www.scratchapixel.com/lessons/3d-basic-rendering/\n",
        "#                ray-tracing-generating-camera-rays/standard-coordinate-systems\n",
        "\n",
        "#     Inputs:\n",
        "#         H, W, focal: image height, width and focal length\n",
        "\n",
        "#     Outputs:\n",
        "#         directions: (H, W, 3), the direction of the rays in camera coordinate\n",
        "#     \"\"\"\n",
        "#     grid = create_meshgrid(H, W, normalized_coordinates=False)[0]\n",
        "#     i, j = grid.unbind(-1)\n",
        "#     # the direction here is without +0.5 pixel centering as calibration is not so accurate\n",
        "#     # see https://github.com/bmild/nerf/issues/24\n",
        "#     directions = \\\n",
        "#         torch.stack([(i-W/2)/focal, -(j-H/2)/focal, -torch.ones_like(i)], -1) # (H, W, 3)\n",
        "\n",
        "#     return directions\n",
        "\n",
        "\n",
        "# def get_rays(directions, c2w):\n",
        "#     \"\"\"\n",
        "#     Get ray origin and normalized directions in world coordinate for all pixels in one image.\n",
        "#     Reference: https://www.scratchapixel.com/lessons/3d-basic-rendering/\n",
        "#                ray-tracing-generating-camera-rays/standard-coordinate-systems\n",
        "\n",
        "#     Inputs:\n",
        "#         directions: (H, W, 3) precomputed ray directions in camera coordinate\n",
        "#         c2w: (3, 4) transformation matrix from camera coordinate to world coordinate\n",
        "\n",
        "#     Outputs:\n",
        "#         rays_o: (H*W, 3), the origin of the rays in world coordinate\n",
        "#         rays_d: (H*W, 3), the normalized direction of the rays in world coordinate\n",
        "#     \"\"\"\n",
        "#     # Rotate ray directions from camera coordinate to the world coordinate\n",
        "#     rays_d = directions @ c2w[:, :3].T # (H, W, 3)\n",
        "#     rays_d = rays_d / torch.norm(rays_d, dim=-1, keepdim=True)\n",
        "#     # The origin of all rays is the camera origin in world coordinate\n",
        "#     rays_o = c2w[:, 3].expand(rays_d.shape) # (H, W, 3)\n",
        "\n",
        "#     rays_d = rays_d.view(-1, 3)\n",
        "#     rays_o = rays_o.view(-1, 3)\n",
        "\n",
        "#     return rays_o, rays_d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "os12SOgSPA1i"
      },
      "outputs": [],
      "source": [
        "# from PIL import Image\n",
        "\n",
        "# def generate_ray_directions(K_inv, width, height):\n",
        "#     # Meshgrid for pixel coordinates\n",
        "#     x = np.linspace(0, width - 1, width)\n",
        "#     y = np.linspace(0, height - 1, height)\n",
        "#     x, y = np.meshgrid(x, y)\n",
        "#     # Homogeneous coordinates of pixels\n",
        "#     pixels = np.stack([x.flatten(), y.flatten(), np.ones_like(x.flatten())], axis=-1)\n",
        "#     # Transform to camera space\n",
        "#     ray_dirs_camera = K_inv @ pixels.T\n",
        "#     ray_dirs_camera = ray_dirs_camera[:3, :].T  # Remove homogeneous coordinate\n",
        "#     # Normalize directions\n",
        "#     norms = np.linalg.norm(ray_dirs_camera, axis=1, keepdims=True)\n",
        "#     ray_dirs_camera /= norms\n",
        "#     return ray_dirs_camera\n",
        "\n",
        "\n",
        "# def load_ground_truth_image(image_path):\n",
        "#     # Load an image file as ground truth\n",
        "#     with Image.open(image_path) as img:\n",
        "#         white_background = Image.new(\"RGB\", img.size, (255, 255, 255))\n",
        "#         rgb_image = Image.alpha_composite(white_background.convert(\"RGBA\"), img).convert(\"RGB\")\n",
        "#     return rgb_image\n",
        "\n",
        "# def get_ray(image_dir, pose, camera_angle_x, H, W):\n",
        "#   trans_mat = np.array(pose)\n",
        "\n",
        "#   f_x = (W / 2) / np.tan(camera_angle_x / 2)\n",
        "#   f_y = f_x\n",
        "#   c_x = W / 2\n",
        "#   c_y = H / 2\n",
        "#   # Intrinsic camera matrix\n",
        "#   K = np.array([\n",
        "#       [f_x, 0, c_x],\n",
        "#       [0, f_y, c_y],\n",
        "#       [0, 0, 1]\n",
        "#   ])\n",
        "\n",
        "#   K_inv = np.linalg.inv(K)\n",
        "\n",
        "#   ray_directions_camera = generate_ray_directions(K_inv, W, H)\n",
        "#   rotation_matrix = trans_mat[:3, :3]\n",
        "#   rays_d = ray_directions_camera @ rotation_matrix.T\n",
        "#   rays_o = np.tile(trans_mat[:3, 3], (W * H, 1))\n",
        "#   ground_truth_px_values = load_ground_truth_image(image_dir)\n",
        "#   reshaped_px_values = np.reshape(ground_truth_px_values, (H*W, 3)) / 255\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#   return rays_o, rays_d, reshaped_px_values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8BlZupIgtP0F"
      },
      "outputs": [],
      "source": [
        "# def load_ground_truth_image(image_path):\n",
        "#     # Load an image file as ground truth\n",
        "#     with Image.open(image_path) as img:\n",
        "#         white_background = Image.new(\"RGB\", img.size, (255, 255, 255))\n",
        "#         rgb_image = Image.alpha_composite(white_background.convert(\"RGBA\"), img).convert(\"RGB\")\n",
        "#     return rgb_image\n",
        "\n",
        "# def get_ray(image_dir, pose, camera_angle_x, H, W):\n",
        "#   trans_mat = np.array(pose)\n",
        "#   f_x = (W / 2) / np.tan(camera_angle_x / 2)\n",
        "\n",
        "#   x, y = np.meshgrid(\n",
        "#             np.arange(W, dtype=np.float32),  # X-Axis (columns)\n",
        "#             np.arange(H, dtype=np.float32),  # Y-Axis (rows)\n",
        "#             indexing='xy')\n",
        "#   homogeneous_directions = np.stack(\n",
        "#             [(x - W * 0.5) / f_x,\n",
        "#             -(y - H * 0.5) / f_x,\n",
        "#             -np.ones_like(x)],\n",
        "#             axis=-1)\n",
        "\n",
        "#   c2w = trans_mat\n",
        "\n",
        "#   rays_d = homogeneous_directions @ c2w[:3, :3].T\n",
        "#   rays_d = rays_d / np.linalg.norm(rays_d, axis=-1, keepdims=True)\n",
        "#   rays_o = np.tile(c2w[:3, 3], (H, W, 1))\n",
        "\n",
        "\n",
        "#   rays_d = rays_d.reshape(-1, 3)\n",
        "#   rays_o = rays_o.reshape(-1, 3)\n",
        "\n",
        "\n",
        "\n",
        "#   ground_truth_px_values = load_ground_truth_image(image_dir)\n",
        "#   reshaped_px_values = np.reshape(ground_truth_px_values, (H*W, 3)) / 255\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#   return rays_o, rays_d, reshaped_px_values"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Acknowledgements"
      ],
      "metadata": {
        "id": "MIpSyfKpYWEm"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}